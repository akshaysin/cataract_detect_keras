{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "c:\\users\\aksha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "c:\\users\\aksha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import os\n",
    "import theano\n",
    "from PIL import Image\n",
    "from numpy import *\n",
    "import cv2 as cv\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read image here in an list\n",
    "path = \"input_path\"\n",
    "listing = os.listdir(path)\n",
    "count=size(listing)\n",
    "print (count)\n",
    "# Validate size of an random image is 128*128\n",
    "im_random=cv.imread(\"input_path//{0}\".format(listing[23]))\n",
    "im_random.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 49152)\n",
      "<class 'numpy.ndarray'>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Convert the image data into an imatrix. imatrix would just be a flat represetation of each image's pixel data in each row.\n",
    "# This imatrix is whats gonna be used to create X_train, X_validation and X_test data.\n",
    "imatrix = np.array([cv.imread(\"input_path//{0}\".format(img)).flatten() for img in listing])\n",
    "# imatrix = []\n",
    "# for img in listing:\n",
    "#     im_reading_now=cv.imread(\"input_path//{0}\".format(img))\n",
    "#     imatrix = np.vstack((imatrix,im_reading_now.flatten()))\n",
    "print (imatrix.shape)\n",
    "print (type(imatrix))\n",
    "print (imatrix.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets create y, i.e labels. This is whats gonna be used to create Y_train, Y_validation and Y_test data.\n",
    "label=np.ones((count,), dtype=int)\n",
    "label[0:1001]=0\n",
    "label[1001:2001]=1\n",
    "size(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets randomnly suffle the data to avoid overfitting\n",
    "data,label=shuffle(imatrix,label,random_state=2)\n",
    "train_data=[data,label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Parameters\n",
    "batch_size = 64\n",
    "nb_classes = 2\n",
    "nb_epoch = 25\n",
    "img_rows, img_col = 128, 128\n",
    "img_channels = 3\n",
    "nb_filters = 32\n",
    "nb_pool = 2\n",
    "nb_conv = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X,y) = (train_data[0],train_data[1])\n",
    "print (X.shape)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting X and y in training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting X_train and y_train in training and validation data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the individual sizes\n",
    "print (\"X_train : {0}\".format(X_train.shape))\n",
    "print (\"y_train :{0}\".format(y_train.shape))\n",
    "\n",
    "print (\"X_val : {0}\".format(X_val.shape))\n",
    "print (\"y_val : {0}\".format(y_val.shape))\n",
    "\n",
    "print (\"X_test : {0}\".format(X_test.shape))\n",
    "print (\"y_test : {0}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the data to pass to CNN\n",
    "X_train = X_train.reshape(X_train.shape[0], 3, 128, 128)\n",
    "X_val = X_val.reshape(X_val.shape[0], 3, 128, 128)\n",
    "X_test = X_test.reshape(X_test.shape[0], 3, 128, 128)\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_val = np_utils.to_categorical(y_val, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the individual sizes\n",
    "print (\"X_train : {0}\".format(X_train.shape))\n",
    "print (\"y_train :{0}\".format(y_train.shape))\n",
    "\n",
    "print (\"X_val : {0}\".format(X_val.shape))\n",
    "print (\"y_val : {0}\".format(y_val.shape))\n",
    "\n",
    "print (\"X_test : {0}\".format(X_test.shape))\n",
    "print (\"y_test : {0}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularize the data\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_val /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model now\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(nb_filters,\n",
    "                        (nb_conv,nb_conv),\n",
    "                        border_mode='valid',\n",
    "                        activation='relu', \n",
    "                        input_shape=(img_channels,img_rows,img_col), \n",
    "                        data_format = 'channels_first'))\n",
    "\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool,nb_pool)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool,nb_pool)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool,nb_pool)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "         verbose=1, validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test this trained model on our test data\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print (\"Test Score :\", score[0])\n",
    "print (\"Test accuracy: \", score[1])\n",
    "print (model.predict_classes(X_test[1:5]))\n",
    "print (y_test[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets save the model to disk\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "model.save(\"whole_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
